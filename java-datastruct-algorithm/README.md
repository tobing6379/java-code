# 数据结构

## 算法

### 递归

递归是一种使用非常广泛的算法（或编程技巧）。只要同时满足以下三个条件，可以使用递归解决。

+ 一个问题的解可以分解成几个子问题的解（子问题就是数据规模更小的问题）
+ 问题与分解之后的子问题，除了数据规模不同，求解思路完全相同
+ 存在递归终止条件

写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。（**写出递推公式，找出终止条件**）

**只要遇到递归，就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。**

如果一个问题A可以分解为若干子问题B/C/D，可以假设子问题B/C/D已经解决，在此基础上思考如何解决问题A。在思考时，只需要思考问题A与子问题B/C/D两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。

编写递归代码时，需要警惕一下几种情况：

+ **警惕堆栈溢出**，JVM中系统栈的大小是有限的，如果递归调用深度太深，会导致堆栈溢出。这时可以通过限制递归调用深度来避免，超过一定深度，直接抛出异常
+ **警惕重复运算**，递归运算时，可以会存在重复计算的问题，为了避免重复计算，可以通过一个数据结构来保存已经求解过的f(k)。当递归调用到f(k)时，先看是否已经求解过。

在时间效率上，递归代码有很多函数调用，当这些函数调用的数量很大时，会积聚成一个可观的时间成本。

在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，因此在分析递归复杂度时，需要额外考虑这部分的开销。

总而言之，递归代码有利有弊。利是递归代码表达力强，写起来简洁；弊是空间复杂度高、有堆栈溢出的风险、存在重复计算、过多函数调用会耗时较多等问题。

需要注意的是，递归本身是通过借助栈来实现，只不过使用的栈是系统或虚拟机本身提供的。我们也可以内存堆上实现栈，手动模拟入栈、出栈过程。

对于递归调试，几乎很难使用IDE的单步跟踪功能，这是可以**通过打印日志，结合条件断点**的方式来进行调试。

### 排序算法

常用的排序算法有：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序等。按照时间复杂度，可以将它们分成三类。

| 排序算法         | 时间复杂度 | 是否基于比较       |
| ---------------- | ---------- | ------------------ |
| 冒泡、插入、选择 | O(n*n)     | :heavy_check_mark: |
| 快排、归并       | O(n*logn)  | :heavy_check_mark: |
| 桶、计数、基数   | O(n)       | :x:                |

评价、分析一个算法可以通过以下几方面入手。

**排序算法的执行效率**

排序算法的执行效率可以从以下几方面来衡量：

+ **最好情况、最坏情况、平均情况时间复杂度**：对于排序的数据，有的接近有序，有的完全无序。有序度不同对排序的执行时间有影响，因此需要知道排序算法在不同数据下的性能表现。
+ **时间复杂度的系数、常数、低阶**：时间复杂度是反映数据规模很多时的增长趋势，因此表示时会忽略系数、常数、低阶。但实际开发中，排序可能是10个、100个等小规模数据，因此，对同一阶时间复杂度的排序算法性能对比时，要考虑系数、常数、低阶。
+ **比较次数和交换次数**：基于比较的排序算法执行过程中，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。因此分析执行效率时，要把比较次数和交换次数也考虑进去。

**排序算法的内存消耗**

算法的消耗的内存可以通过时间复杂度来衡量，排序算法亦是如此。对于排序算法，还引入了新概念，**原地排序**，用于特指时间复杂度为O(1)的排序算法。

**排序算法的稳定性**

排序算法中，还有一个重要的指标，稳定性。稳定性指的是，如果待排序的序列中存在值相等的元素，经过排序后，相同元素之间原有的先后顺序不变。

<font style="color:red">**为什么需要考虑稳定性？**</font>

> 在时间的项目开发中，需要排序的不仅仅是单纯的整数，而是一组对象，我们需要按照对象的某个key来排序。如订单系统中的“订单”有两个属性，一个是下单时间，另外一个是订单金额。如果需要按照金额大小进行排序，对于金额相同的订单，按照下单时间从早到晚排序，需要怎么实现呢？
>
> 最容易想到的是，先按照金额对订单进行排序，然后再遍历之后的订单数据，对于每个金额相同的小区间按照下单时间排序。这种排序思路简单，但是实现起来比较复杂。
>
> 但是如果使用稳定算法，实现起来就非常简洁。如先按照订单下单时间排序，排序完成之后，在通过稳定排序算法，按照订单金额重新排序。两遍排序之后，就可以的上述的效果。
>
> 除此之外，在平时我们使用Windows资源管理器中，我们可能会先按照文件名排序，再按照文件类型排序，这样一来，就可以得到按文件类型排序，相同文件类型的按照文件名排序。这样也是利用了排序算法的稳定性。

#### 冒泡排序

冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻元素进行比较，看是否满足大小关系。如果不满足就进行交换。一次冒泡至少让一个元素移动到它应该的位置，重复n次，就完成了n个数据的排序工作。

上述的冒泡排序过程还可以进行优化，即当某次冒泡操作已经没有数据交换时，说明到达完全有序，不用再继续执行后续的冒泡操作。

**原地算法**：冒泡过程只涉及相邻数据的交换操作，只需要常量级的临时空间，因此空间复杂度为O(1)。

**稳定算法**：在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证算法稳定，当相邻元素大小相等时，不做交换。

**最好最坏时间复杂度**：最好情况，要排序数据已经有序，只需要1次冒泡操作即可结束，因此最好时间复杂度为O(n)。最好情况，排序数据时完全无序，需要进行n次冒泡操作，因此最好时间复杂度为O(n*n)。

**平均时间复杂度**：平均时间复杂度即加权平均期望时间复杂度，如果使用概率论分析起来会比较复杂，可以通过“**有序度**”和“**逆序度**”进行分析。

+ **有序度是数组中有序关系的元素对个数。**对于一个倒排数组，如6，5，4，3，2，1，有序度是 0；对于一个完全有序的数组，比如 1，2，3，4，5，6，有序度就是n*(n-1)/2，也就是 15。我们把这种完全有序的数组的有序度叫作满有序度。  
+ **逆序度 = 满有序度 - 有序度**  

冒泡排序中包含两个操作原子，比较和交换。没交换一次，有序度加1。不管算法怎么改进，交换次数总是确定，**即为逆序度，即n*(n-1)/2-初始有序度。**

最坏情况下，初始状态的有序度是 0，所以要进行 n\*(n-1)/2 次交换。最好情况下，初始状态的有序度是 n\*(n-1)/2，就不需要进行交换。我们可以取个中间值 n\*(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。  即平均时间复杂度为O(n*n)

#### 插入排序

对于一个有序的数组，如果需要保证插入一个新元素仍然有序，可以在插入时遍历数组，找到数据应该插入的位置插入即可。

可以看出上面是一个动态的过程，保证动态插入的数据之后，数组仍然有序。对于一组静态的数据，也可以借鉴上述方法，进行插入排序，那就是插入排序算法的核心实现。

实现将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，即数组的对一个元素。插入算法的核心思想是**取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并且保证已排序区间数据一直有序，重复这个过程**。知道未排序区间中元素为空，算法结束。

插入排序中包含两种操作：元素比较和元素的移动。当需要将一个数据a插入到已排序区间时，需要拿a和已排序区间的元素依次比较大小，找到合适位置。找到合适位置之后，需要将插入点之后的元素顺序往后移动一位，腾出位置给元素a插入。（如果对于链表，可以不用挪动元素）

对于不同的查找插入点方法（从头到尾、从尾到头），元素比较的次数是有区别的，但对于一个给定的初始序列，移动操作的次数总是固定的，等于逆序度。

**原地算法**：插入排序不需要额外的空间，因此空间复杂度是O(1)。

**稳定算法**：插入排序中，对于值相同的元素，可以选择将其后面出现的元素，插入到前面出现的元素后面，可以保证前后有序性不变，是一种稳定算法。

**最好最坏时间复杂度**：最好情况，元素有序，不需要搬动任何数据，如果是从头到尾在有序数据中查找插入位置，每次只需要比较一个数据就能确定插入的位置。因此最好时间复杂度为O(n)。最好情况，数组倒序，每次插入相当于在数组第一个位置插入新的数据，移动大量元素。因此最坏时间复杂度为O(n*n)。

**平均时间复杂度**：在数组中插入一个数据的平均时间复杂度是 O(n)，对于插入排序，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以平均时间复杂度为 O(n*n)

#### 选择排序

选择排序思路类似于插入，也将数组分为排序区间和未排序区间。但须选择排序每次会从未排序区间找到最小元素，将其放到已排序的区间尾部。

**原地算法**：显然选择排序可爱内分组的为O(1)。

**非稳定算法**：可以注意，选择排序每次要找剩余元素中最小值，并和前面的元素交换位置。从最小元素位置到要交换位置元素之间，开年会存在于被交换元素相等的数据，因此交换操作会破坏稳定性。

**时间复杂度**：选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n )。

<font style="color:red">**为什么插入排序优于冒泡排序？**</font>

不论如何，冒泡排序的元素交换次数都是一个固定值，为原始数据的逆序度。插入排序也是，不论怎么优化，元素移动的次数等于元素数据的逆序度。

但从代码实现角度，冒泡排序的数据交换要比插入排序的数据移动要复杂。（冒泡排序需要3个赋值操作，插入操作需要1个）

> 补充：[插入排序的优化](https://zh.wikipedia.org/wiki/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F)

| 排序算法 | 是否原地排序       | 是否稳定           | 最好   | 最坏   | 平均   |
| -------- | ------------------ | ------------------ | ------ | ------ | ------ |
| 冒泡排序 | :heavy_check_mark: | :heavy_check_mark: | O(n)   | O(n*n) | O(n*n) |
| 插入排序 | :heavy_check_mark: | :heavy_check_mark: | O(n)   | O(n*n) | O(n*n) |
| 选择排序 | :heavy_check_mark: | :x:                | O(n*n) | O(n*n) | O(n*n) |

在三种时间复杂度为O(n*n)的排序算法中，冒泡排序、选择排序，可能就纯粹停留在理论的层面，而插入排序还是挺有用的。  

![base_sorte_exec_principal](https://gitee.com/tobing/imagebed/raw/master/base_sorte_exec_principal.png)

#### 归并排序

对于要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。  

> 分治思想和递归思想很像，分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧。

**稳定算法**：归并算法稳定性关键看合并过程的merge()函数，在合并过程中对于相同值，可以优先把前面部分先放到临时数组，就可以保证合并前后顺序不变。

**时间复杂度**：归并排序涉及递归，时间复杂度的分析稍微有点复杂。可以发现递归的时间复杂度可以写成递推公式。归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是(nlogn)。  

**空间复杂度**：归并排序并不是原地排序，在每次递归的时候需要额外的数组来储存每次合并的结果。而这些临时的空间会在每次使用完毕会释放，因此并不需要累加，同时这些空间占用不会超过n个数据大小，因此空间复杂度为O(n)。

```c
// 归并排序算法, A 是数组，n 表示数组大小
merge_sort(A, n) {
	merge_sort_c(A, 0, n-1)；
}
// 递归调用函数
merge_sort_c(A, p, r) {
    // 递归终止条件
    if p >= r then return
        
    // 取 p 到 r 之间的中间位置 q
    q = (p+r) / 2
    // 分治递归
    merge_sort_c(A, p, q)
    merge_sort_c(A, q+1, r)
    // 将 A[p...q] 和 A[q+1...r] 合并为 A[p...r]
    merge(A[p...r], A[p...q], A[q+1...r])
}
```

#### 快速排序

同样地，快排也是基于分治算法实现。

对于要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将pivot 放到中间。

经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。  

```c
// 快速排序，A 是数组，n 表示数组的大小
quick_sort(A, n) {
	quick_sort_c(A, 0, n-1)
}
// 快速排序递归函数，p,r 为下标
quick_sort_c(A, p, r) {
	if p >= r then return
	
	q = partition(A, p, r) // 获取分区点
	quick_sort_c(A, p, q-1)
	quick_sort_c(A, q+1, r)
}
```

>partition()分区函数就是随机选中一个元素作为pivot的下标，然后对A[p...r]分区，函数返回pivot下标。
>
>如果不考虑空间消耗，partition()可以编写的很简单，申请两个临时数组X和Y，遍历遍历 A[p…r]，将小于 pivot 的元素都拷贝到临时数组 X，将大于 pivot 的元素都拷贝到临时数组 Y，最后再将数组 X 和数组 Y 中数据顺序拷贝到 A[p…r]。  
>
>如果希望快排是原地算法，那么partition()分区函数就不能占用太多额外空间，需要在A[p…r]原地完成。这个过程有点类似选择排序。通过游标 i 把 A[p…r-1] 分成两部分。A[p…i-1] 的元素都是小于 pivot 的，我们暂且叫它“已处理区间”，A[i…r-1] 是“未处理区间”。我们每次都从未处理的区间 A[i…r-1] 中取一个元素 A[j]，与 pivot 对比，如果小于 pivot，则将其加入到已处理区间的尾部，也就是 A[i] 的位置。 

**原地算法**：由上面的分区函数可以会知道，快排可以是原地算法。

**不稳定算法**：从分区的过程可以看出，快排是不稳定的算法。

**时间复杂度**：每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是 O(nlogn) 。

归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。  

<font style="color:red">**如何利用快排的思想以O(n)时间复杂度内求无序数组中的第K大元素？**</font>

+ 将数组区间A[0...n-1]的最后一个元素A[n-1]作为pivot，对数组A[0...n-1]原地分区，将数组分成三部分：A[0...p-1]、A[p]、A[p+1...n-1]。

+ 如果p+1=K，则A[p]就是要求的元素；
+ 如果K>p+1,说明第K大元素出现在A[p+1...n-1]区间，重复第一步在A[p+1...n-1]区间查找。
+ 同理，如果K<p+1，说明第K大元素出现在A[p+1...n-1]区间，重复第一步在A[0...p-1]区间查找。

在上述的过程中，第一次分区查找时，需要对大小为 n 的数组执行分区操作，需要遍历 n 个元素。第二次分区查找时，只需要对大小为 n/2 的数组执行分区操作，需要遍历 n/2 个元素。依次类推，分区遍历元素的个数分别为、n/2、n/4、n/8、n/16.……直到区间缩小为 1。

**快排优化**  

当需要排序的数据接近有序，如果每次分区点都选择最后一个数据，那么快排的时间复杂度会退化为O(n*n)。为了能够保证总体的分区是平衡的，从而达到O(nlogn)的时间复杂度，需要合理的选择分区点（合理：被分区点分开的两个分区中，数据的数量差不多）。

显然，粗暴地选择第一个或最后一个数据作为分区点都不是明智的。常用、比较简单的分区算法有：

+ 三数取中法：从区间的首、尾、中间分别取出一个树，然后对比大小，取3个数的中间值作为分区点。如果数据量较大，还可以进行“五数取中法”或“十数取中法”
+ 随机法：每次从排序区间，随机选择一个元素，作为分区点。这种虽然无法保证每次选择的分区点都是较好的，但能够在概率上保证整体上而言分区点是较好的。

#### 桶排序

桶排序核心思想是将要排序的数据放到几个有序的桶中，每个桶内数据单独排序。桶内数据排序完毕，再把每个桶的数据按序一次取出，组成的序列就是有序的。

**时间复杂度分析**：如果要排序的数据有n个，将其**均匀**分到m个桶中，每个桶有k=n/m个元素。每个桶内部使用快排/归并等高级排序，时间复杂度为O(k\*logk)。m个桶的时间复杂度就是O(m\*k\*logk)，因为k=n/m，因此整个桶排序时间复杂度为O(n\*log(n/m))。当**m接近于n时，即桶的个数与数据规模相近时**，log(n/m)是有个非常小的常量，这时桶排序时间复杂度接近于O(n)。

**使用条件苛刻**：从上面可以看出，桶排序使用的条件相当苛刻。**一方面**，要排序的数据需要容易分成m个桶，并且桶与桶之间有着天然的大小顺序。这样可以保证桶内排序之后，桶与桶之间不需要再排序。**另一方面**，数据在桶内的分布是比较均匀的，如果桶划分之后，桶的数据分布很不均匀，那么桶内的数据排序时间复杂度将不再是常量级。极端情况下，数据都被划分到一个桶中，会退化为O(nlogn)。

**适合外部排序**：桶排序适用于数据量很大，无法将所有数据全部加载到内存中的场景。

> 比如有10GB订单数据，希望按照订单金额排序，但内存有限（几百MB），无法一次性将10GB数据加载到内存。这时，可以先扫描一遍文件，确定数据范围（如最小金额1元，最大为10万元）。
>
> 将订单根据金额划分到100个桶中，第一个桶储存1~1000元，第二个桶储存1001~2000元，以此类推。每个桶对应一个文件，并安装金额范围的大小顺序编号命名。
>
> 理想情况下，订单金额在1到10万之间，均匀分布，则订单数据均匀划分到100个文件，每个文件大约100MB。可以将这100MB数据放到内存中，使用高级排序算法（如快排，归并）进行排序。等每个文件都排序完，最终将所有文件中的数据按文件编号依次取出，合并成一个文件，就完成了排序。
>
> 在非理想情况，数据无法均匀划分到100个文件中，可以某个金额区间的数据特别多，划分出的数据很大，无法一次性写入文件。这时可以针对数据很大的文件继续划分，直到所有文件都能读入内存为止。

#### 计数排序

计数排序可以看做桶排序的特殊情况。当要排序n个数据，数据的范围不大时，如最大值为k，可以把数据分为k个桶。每个桶的数据值都是相同的，可以省略掉桶内排序的时间。

如高考50万考生，考生的考分范围是0~900分，数据范围很小，因此可以分为901个桶，分别对应0~900分。根据考生成绩，对50万考生划分到901个桶。桶内的数据都是分数相同的考生，因此桶内不需要进行排序。这样只需要依次遍历每个桶，将桶内考生一次输出到一个数组，可以实现50万考生的排序。因为只涉及遍历操作，时间复杂度为O(n)。

假设有8个考生，分数在0到5分，考生成绩分别为：A[8]={ 2，5，3，0，2，3，0，3 }。

1. 扫描考生成绩A[8]，统计每个分数的计数，放到数组C[6]中
2. 对C[6]，从左到右顺序求和，C[k]储存小于等于分数k的考生个数
3. 从后往前依次遍历数组A。如当扫描到3，从C[6]中取出下标为3的值
4. C[6]中下标为3的值，代表了小于等于3的考生个数：7
5. 即第一个扫描到分数为3的考试生排在第7位，直接将其放到新数组R[8]第7个位置
6. 当将分数为3的考试放到新数组之后，剩下的元素中只有6个考试分数小于等于3
7. 此时，需要将C[6]中下下标为3的考生个数进行更新，即减去1

![bucket_sort_exec_principal](https://gitee.com/tobing/imagebed/raw/master/bucket_sort_exec_principal.png)

计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据、n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。

> 此处需要注意，从后往前的目的是为了保证**稳定性**。

#### 基数排序

假如排序的数据字段长度为k位，依次根据数据字段的第k位，第k-1位，...，第1位依次使用**稳定**线性排序算法进行排序，最终就可以实现按照该字段排序。

在上面的算法中，稳定是关键。就像前面提到的，稳定才能保证在按照最后一次排序的顺序基础上，对于相同元素的，按照倒数第二次排序的顺序进行排序。

在基数排序中，如果排序的数据有k位，需要执行k次线性排序（如桶排序或技术排序），它们的数据复杂度是O(k*n)，当k不大时，如手机号排序的场景，k为11，因此基数排序的时间复杂度接近于O(n)。

对于要排序的数据长度不相等的情况，可以想办法将其补充到相同的长度，再进行基数排序。

总而言之，基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。 

#### 选择合适的排序算法

| 排序算法 | 时间复杂度        | 是否稳定           | 是否原地           |
| -------- | ----------------- | ------------------ | ------------------ |
| 冒泡排序 | O(n*n)            | :heavy_check_mark: | :heavy_check_mark: |
| 插入排序 | O(n*n)            | :heavy_check_mark: | :heavy_check_mark: |
| 选择排序 | O(n*n)            | :x:                | :heavy_check_mark: |
| 快速排序 | O(nlogn)          | :x:                | :heavy_check_mark: |
| 归并排序 | O(nlogn)          | :heavy_check_mark: | :x:                |
| 计数排序 | O(n+k)k树数据范围 | :heavy_check_mark: | :x:                |
| 桶排序   | O(n)              | :heavy_check_mark: | :x:                |
| 基数排序 | O(dn)d为维度      | :heavy_check_mark: | :x:                |

线性排序算法时间复杂度比较低，适用场景比较特殊。

对于小规模数据的排序，可以选择时间复杂度是O(n*n)的算法；

对于大规模数据的排序，时间复杂度是O(nlogn)的算法更加高效；

对于内存空间比较敏感的场景，选择排序算法时，需要注意算法是否为原地算法。

**排序算法在实际中的运用**

在Glibc中的qsort函数中，

+ 对于小数据量的排序，比如 1KB、2KB 等 ，优先使用归并排序（考虑到归并排序稳定）
+ 对于数据量大的排序，如 100MB 等，改为快速排序算法实现（考虑到归并不是原地算法）
+ 在快排的过程中，当排序的区间元素个数小于等于4时，会使用插入排序（小数据规模，O(nlog)不见得就会比O(n*n)要好，要考虑系数问题）
+ 在使用快速排序时，分界点的选定使用“三数取中法”
+ 对于递归太深导致堆栈溢出问题，qsort通过自己实现一个堆上栈，手动模拟递归来解决

### 二分思想

利用二分思想，每次都有序元素的中间数据进行比较，每比较一次，可以舍弃掉一半数据，效率非常高。

**时间复杂度**：对与二分查找，每次都可以舍弃一半元素，时间复杂度为O(logn)。假如需要缩小k次，这数据规模为n/(2^k)=1，可以求得k=log2(N)。

> 需要注意，由于大O标记法在表示时间复杂度的时候，会忽略掉常数、系数和低阶。对于常数级时间复杂度，O(1)有可能是一个非常大的常量值，因此厂里数据复杂度的算法有可能没有O(logN)的算法执行效率高。

**严苛的使用场景**

尽管二分法的执行效率很高，但是需要依赖严苛的应用场景。

+ 二分法依赖顺序表（如数组），更加具体的就是依赖顺序表的随机访问
+ 二分法查找针对的是有序数据，如果数据无序，则需要先排序
+ 数据量太小不适合二分法，数据量很小可以直接使用顺序遍历
+ 但如果数据之间比较比较耗时，遍历会比较慢，使用二分法可以减少比较次数
+ 数据量太大不适合二分法，二分法依赖数组，数组依赖连续的内存空间，太大的连续内存空间是不容易获得的

> 查找数据为什么要先排序再使用二分法呢？排序的过程不就可以找到那个元素了吗？
>
> 对元素排序再使用二分法是针对于需要多次查找元素的场景，如果只需要查找一次，可以直接采用遍历，效率更高。

**简单但不易掌握**

虽然二分法的思想理解起来并不困难，但是需要编写出比较完美的二分法并不简单。

即使是在编写有序数组不存在重复元素的情况，也需要注意下面3个易出错的地方。

+ 循环退出条件：是 low<=high，而不是 low<high。
+ mid的取值：mid=(low+high)/2的写法，low和high有可能导致和溢出，可以写成low+(high-low)/2或者low+((high-low)>>1)，位运算比除法运算效率更高。
+ low和high的更新：low=mid+1，high=mid-1，不能是high=mid或low=mid，否则可能发生死循环

**二分法变体一**

有序集合中存在重复元素，查找第一个值等于给定值的元素。

这种变体的关键是，对于元素相等的情况，需要额外处理。

```java
if (a[mid] == value) {
    if (mid == 0 || (a[mid - 1] != value)) {
        // 如果找到元素是数组第一个或前面的元素不等于value，说明是第一个符合元素
        return mid;
    } else {
        // 否则，往左区间继续找
        high = mid - 1;
    }
}
```

**二分法变体二**

有序集合中存在重复元素，查找最后一个值等于给定值的元素。

这个变体和上面的类似，还是对于元素相等的情况，需要额外处理。

```java
if (a[mid] == value) {
    if (mid == n - 1 || (a[mid + 1] != value)) {
        // 如果找到元素是数组最后一个或后面的元素不等于value，说明是最后一个符合元素
        return mid;
    } else {
        // 否则，往右区间继续找
        low = mid - 1;
    }
}
```

**二分法变体三**

查找第一个大于等于给定值的元素。

思路和上面类似，代码如下。

```java
if (a[mid] >= value) {
    if (mid == 0 || (a[mid - 1] < value)) {
        // 元素已经是第一个元素了，后面元素肯定比value大
        // 元素的前面元素比当前元素要小，则该元素是第一个大于等于value的
        return mid;
    } else {
        high = mid - 1;
    }
}
```

**二分法变体四**

查找最后一个小于等于给定值的元素。

这个思路和上面一致。

```java
if (a[mid] <= value) {
    if (mid == n - 1 || (a[mid + 1] > value)) {
        // 元素已经是最后一个元素了，前面元素肯定比a[mid]小
        // 元素的后面元素比当前元素要大，则该元素是第一个小于等于value的
        return mid;
    } else {
        low = mid - 1;
    }
}
```

### 哈希算法

在数据结构的[散列表](# 散列表)中，使用到了哈希算法来将数据的键值转换为数组下标。

哈希算法可以将任意长度的二进制值串映射为固定长的的二进制值串，这个映射规则就是哈希算法，通过原始数据映射之后得到的二进制数据就是哈希值。

一个优秀的哈希算法需要符合以下几点要求：

+ 从哈希值不能反向推到出原始数据（单向性）
+ 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同（敏感性）
+ 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小（冲突概率小）
+ 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值（运算高效）

哈希算法使用场景众多，最常见的是：安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分配、分布式储存。

**安全加密**

哈希算法最常用于安全加密，如MD5、SHA、DES、AES等。

对于用于加密的哈希算法而言，**单向性**和**冲突概率小**两种特性格外重要。

在加密算法中，安全性与性能是相互制衡的。一般而言越安全的算法，其运算时间也会越长。

> 理论上讲，只能尽可能降低冲突，无法做到完全不冲突。其中原理基于组合数学理论-鸽巢原理。即一个n个鸽巢，如果每个鸽巢只能存放一个鸽子，肯定无法容纳n+1个鸽子。同理，对于哈希算法，其产生的哈希值长度是固定的，而我们输入的数据是无穷大，这样就肯定会存在哈希冲突的情况。因此，一般而言，哈希值越长的算其散列冲突概率越低。

**唯一标识**

如在海量的图库中，搜索一张图片是否存在不能仅通过图片的元数据（如图片名称来对比），因为可能存同名但内容不同或者名称不同但图片内容相同的情况。

由于文件在计算机中都表示为二进制串，因此比较笨的部分是比较两张图片的所有二进制串是否相同。但由于图片转换成的二进制串都比较长，比较起来相当耗时。

通过哈希算法，可以活动图片而二进制串的消息摘要（唯一标识）。如从图片二进制串开头、中间、最后各取100字节，同哈希算法得到一个哈希字符串，作为图片的消息摘要。通过比较这个消息摘要，可以判断图片是否存在图库中。

**数据校验**

在使用BT下载软件的时候，会将文件分割为很多文件块传输，这些文件块可以从多个机器并行下载，等下载完毕再合并组织成一个文件。

但网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的。这时可以通过哈希算法，对文件取哈希值，并将其保存在种子上。由于哈希算法非常敏感，能检测出微小修改，当文件下载完毕对其进行哈希算法运算，将结果在对照种子文件上的值就可以判断是否被修改。

**散列函数**

散列表中的散列函数不叫看重**数据分布的均匀性**以及**执行效率**。

**负载均衡**

负载均衡算法多种多样，如轮询、随机、加权轮询等。而通过哈希算法可以实现session sticky的负载均衡算法。（session stick，粘滞，即同一客户端上，一次会话中的所有请求都路由到同一个服务器上）

可以同哈希算法，对客户端IP地址或会话ID计算哈希值，将获得的哈希值与服务器列表大小进行取模运算，最终得到的值就是被路由到的服务器编号。这样一来，对于同一个IP/sessionID的所有请求，都会被路由到同一个台后端服务器上。

**数据分片**

> 大日志分析

假如需要对一个1TB的日志文件进行分析，统计关键词被搜索的次数。这时会存在两大问题，一是少数人在很多，无法将其放到一台机器的内存中；二是只用一台机器出了巨大的数据，处理时间很长。

于是可以先对数据分片，然后采用多态将其处理的方法来提高处理的速度。为了提高效率，使用n台机器并行处理。从日志文件中，依次读出每个搜索关键字，同哈希函数计算哈希值，然后多n取模，最终得到的值就是应该被分配到的机器编号。

这样，哈希值相同的搜索关键词被分配到同一条机器。即同一关键词会被分配到同一个机器，每个机器分别计算关键词出现的次数，最后合并起来就是最终结果。

上述过程是MapReduce的基本设计思想。

> 海量图片储存

假如有1亿张图片，在单机上建立散列表是行不通的。可以准备n条机器，每台机器负责维护一部分的图片对一个的散列表。可以每次从图库中读取一个图片，计算唯一标识，然后与基础个数n取模，得到分配的机器编号，之后图片的唯一标识和图片路径会发往相应机器构建散列表。

当需要判断一个图片是否存在于图库，通过哈希算法，计算图片的唯一标识，与机器树n取模得到图片所在机器，在机器的散列表上查找。

**分布式储存**

互联网的数据是海量的，无法通过单节点来储存。因此对于海量的数据，需要将数据分布储存到多台机器上。

可以采用数据分片的方式来决定将数据放到哪个机器上，即通过哈希算法对数据取哈希值，然后对机器个数进行取模，最终的值就是数据应该储存的编号。

但是这种解决方案会存在一个问题，即需要扩容时，会增加机器的数量。这时就仅仅是添加了新的机器，数据分片时需要对机器数取模再决定数据存储的位置，当集群数量改变，原有的数据都需要重新执行数据分配过程，放到新的机器上，这样一来，以前的数据就一下子全部失效了。

为了避免大量的数据搬移，需要使用到**一致性哈希算法**。

> 基本思想：假设我们有 k 个机器，数据的哈希值的范围是 [0, MAX]。我们将整个范围划分成 m 个小区间（m 远大于 k），每个机器负责 m/k 个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。

[一致性哈希算法](https://zh.wikipedia.org/wiki/%E4%B8%80%E8%87%B4%E5%93%88%E5%B8%8C)

[扩展阅读](https://segmentfault.com/a/1190000021199728)

### 字符串匹配

字符串匹算法有很多，根据匹配的串的个数，可以分为单模式匹配和多模式匹配。

+ 单模式串匹配算法即一个串跟一个串进行匹配；常见的单模式匹配算法有：BF算法、RK算法、BM算法、KMP算法；
+ 多模式串匹配算法即一个串中同时查找多个串。常见的多模式匹配算有：Trie树、AC自动机。

#### 概念介绍

在字符串匹配中，为了便于描述，将定义以下的概念。

+ 主串：在字符串A中查找字符串B，字符串A就是主串
+ 模式串：在字符串A中查找字符串B，字符串B就是模式串

#### BF算法

BF算法，Brute Force算法，暴力匹配算法。

在主串中，检测起始位置分别时0、1、2...n-m且长度为n-m+1个字串，看是否有跟模式串匹配的。

BF算法的思路简单，最坏的时间复杂的为O(n\*m)。尽管理论上BF算法的时间复杂度很高，但是在工程实践中，BF算法确是一个比较常用的字符串匹配算法，原因主要有：

+ 实际工程中，模式串和主串的长度不会太长，而且每次模式串与主串匹配时，当中途遇到不能匹配的字符时就可以停止。因此大部分情况下，算法的执行效率要比理论值高很多
+ BF算法实例简单，代码实现简单，不容易出错。在工程中，符合KISS设计原则。

#### RK算法

RK算法，Rabin-Karp算法，是BF算法的升级版。

在BF算法中，加入模式串长为m，主串长为n，在主串中就会有n-m+1个长度为m的子串，只需要暴力对比n-m+1个子串与模式串就可以找出主串与模式串匹配的子串。

在BF算法的执行过程中，每次都需要检查主串与子串是否匹配，需要一次对比每个子串，因此BF算法的时间复杂度比较高，为O(n\*m)。**RK算法基于这一点，引入哈希算法，降低比较的时间复杂度**。

RK算法的思路是，同哈希算法对主串中的n-m+1个子串分别求哈希值，然后逐个与模式串比较大小。如果某个子串的哈希值与模式串相等，说明对应的子串和模式串匹配。以你为哈希值是一个数字，数字之间的比较非常快速，因此模式串与子串比较的效率就提高了。

但是在构建子串的哈希值的时候，仍然需要遍历子串中的每个字符。经过模式串与子串比较的效率提高了，但是算法的整体效率没有提高。因此为了能够整体地提高效率，必须要提高构建子串哈希值的效率。

为了提高子串哈希值的构建效率，可以设计这样一个哈希算法：假设匹配的字符串的字符集中包含了K个字符，可以用一个K进制数来表示一个子串，这个K进制数黄钻和为十进制数作为子串的哈希值。如要处理的字符串只包含a~z共26个小写字母，可以使用二十六进制来表示这个字符串，然后在转换为十进制作为哈希值。

对于上面提到的哈希算法，在相邻的两个子串的哈希值的计算公式有一定的关系。

![image-20210816105728102](https://gitee.com/tobing/imagebed/raw/master/image-20210816105728102.png)

这样一来，通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出主串的哈希值，因此这部分的时间复杂度为O(n)。

而模式串与每个子串的哈希值之间的比较的时间复杂度为O(1)，一共需要比较n-m+1个子串的哈希值，因此总的时间复杂度为O(n)。

> 需要注意，在上面提到的哈希值算法中，如果模式串很长，相应的子串也会很长，进而导致运算得到的哈希值很大，可能会超出int的表示范围。此时可以适当允许散列冲突的存在。

#### BM算法

BM算法，Boyer-Moore算法，主要通过跳过一些无意义的比较来提供字符串匹配的效率。

为了能够跳过无意义的比较，BM算法提供了下列规则：

+ 坏字符规则：当遇到一个不匹配，移动模式串直到不匹配变为匹配；
+ 好后缀规则：当我们移动模式串时，确保在上一个对齐中匹配的字符也在下一个对齐中匹配
+ 反向比较：传统的暴力匹配算法中，模式串移动方向与模式串中字符比较方向一致；而BM算法在进行比较过程中，模式串移动方向与模式串中字符比较方向相反。

![image-20210816112307422](https://gitee.com/tobing/imagebed/raw/master/image-20210816112307422.png)

**坏字符规则**

当遇到一个不匹配时，移动模式串，直到这个不匹配变成匹配。

![image-20210816112818487](https://gitee.com/tobing/imagebed/raw/master/image-20210816112818487.png)



**好后缀法则**

在遇到一个不匹配时，在坏字符之后部分都是匹配的，对于这部分匹配的字符串，将其称为好后缀，记为`t`。

对好后缀`t`，可以将其放到模式串中查找，如果在模式串中能够找到相同的，就将模式串找到的`t‘`与子串中的`t`进行对齐。

![image-20210816123220541](https://gitee.com/tobing/imagebed/raw/master/image-20210816123220541.png)

**坏字符与好前缀的综合运用**

细心的可以发现，对于好后缀的场景，坏字符仍然适用。但是为什么还需要使用好后缀呢？

原因其实很简单，相比于一个字符(坏字符规则对齐)，多个字符(好前缀规则对齐)在模式串中出现的概率要更小，可能完全跳过前面字符的概率也就更大。

综上所述，对于好后缀和坏字符这两个规则，在组合使用时，需要评估两种对于跳过无意义比较的收益大小，选择最有优的。如下图。

![image-20210816124537964](https://gitee.com/tobing/imagebed/raw/master/image-20210816124537964.png)

[参考资料](http://www.cs.jhu.edu/~langmea/resources/lecture_notes/04_boyer_moore_v2.pdf)

**算法实现**

对于坏字符规则，需要在模式串中**定位坏字符所在的位置**，进而确定需要跳过的字符数。

对于好后缀规则，同样需要在模式串中**定位好后缀所在的位置**，从而确定需要跳过的字符数。

如果是采用的是每次顺序遍历查找的方式，就会导致比较低效，从而影响算法的性能。

//todo

#### KMP算法

//todo

#### Trie树

### 贪心算法

经典场景：赫夫曼编码、Prim、Kruskal最小生成树、Dijkstra单源最短路径算法。

**贪心算法解决问题的步骤**

1. 针对一组数据，定义了限制只和期望值，希望从中选出几个数据，在满足限制值的前提下，期望值最大。

2. 每次选择当前情况下，在对限制值同等贡献的情况下，对期望值贡献最大的数据。
3. 大部分情况下，可以举几个例子看贪心算法的结果是否最优。

#### 使用场景

**装豆子**

有可以容纳100KG物品的背包，有5中豆子，每种豆子的总量与价值都各不相同。为了让背包中装的总价值量最大，然后选择再背包中装那些豆子？每种豆子装多少？

运用贪心算法：将5种豆子单价从高到低排序，每次只选择当前情况下单价最高的豆子，直到背包被装满。

**分糖果**

要把m个糖果分给n个孩子，每个孩子只能分一个糖果（m<n）。但是由于糖果少，孩子多，因此糖果只能分给部分孩子。

+ 在m个糖果中，每个糖果大小不等，分别是s1，s1，s3，.......，sm；

+ 对于n个孩子，每个孩子对糖果大小的需求不一样，只有糖果大小大于等于孩子需求，孩子才能够满足。加入孩子对糖果大小的需求分别是g1，g2，g3，.......，gn；

现在需要给出一个分配策略，尽可能满足最大数量的孩子。

运用贪心算法：对于一个孩子，只分配满足需求的最小糖果，这样一来，较大的糖果可以分配给需求更大的孩子；对于不同孩子，优先分配对糖果需求小的孩子，因为他们的需求比较容易满足，同时满足一个需求小的孩子跟满足一个需求大的孩子对期望值的贡献值是一致的。

**钱币找零**

有1 元、2 元、5 元、10 元、20 元、50 元、100 元面额的纸币，张数分别是c1、c2、c5、c10、c20、c50、c100。现在需要用这些钱来支付K元，最少需要多少张纸币？

在生活中，我会先用面值大的支付，如果不够，就继续使用更小一点面值的，以此类推，最后剩下的用1元补齐，这也是一种贪心思想的体现。

**区间覆盖**

假设n个区间，区间的起始端点和结束端点分别是 [l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]。需要从n个区间中选出部分区间，这部分区间满足两两不相交，最多能选出多少个区间？

运用贪心思想：每次选择的时候，选择左端点跟前面已经覆盖的区间不重合，右端点尽量小，这样可以让剩下的未覆盖区间尽可能的大，可以放置更多的区间。

**赫夫曼编码**

有一个包含1000个字符的文件，每个字符占用1Byte，储存1000个字符需要1000Byte。

在这个1000个字符中，统计分析发现只包含6种不同字符，假设分别是a/b/c/d/e/f，且发现它们的频次不同。

赫夫曼编码通过考察文本中的字符种类，以及每种字符出现的频率，根据频率选择不同长度的编码。根据贪心思想，把出现频率较多的字符，用稍短的编码；出现频率较少的字符用稍微长一些的编码。（为了避免解压缩过程中的歧义，赫夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。）

**不适用场景**

在有权图中，从顶点S开始，找一条到顶点T的最短路径。

运用贪心算法：每次选择一条跟当前顶点相连的权最小的边，直到找到顶点T。

按照这种思路，求解得到的路径不是最优解，因此这种问题不能使用贪心算法，因为这种问题中「前面的选择，会影响后面选择的结果」。因为可能存在，第一步选择最优，但是选择的最优第一步导致后面的每一步选择都很糟糕，导致与全局最优解无缘。

### 分治算法

分治算法的核心实现是「分而治之」，即将原问题划分为n个规模较小，并结构与原问题相似的子问题，递归地解决这些子问题，然后合并其结果，就可以得到原问题的解。

分治算法与递归很想， 但分治算法是一种处理问题的思想，递归是一种编程技巧。分支算法往往会通过递归来实现。分支算法的递归实现中一般会设计是三个操作：

+ ①分解，将原问题分解为一些系列子问题；
+ ②解决：递归求解子问题，若子问题足够小可以直接求解；
+ ③合并：将子问题的结果合并为原问题。

分治算法使用时，需要满足以下几个条件：

+ 原问题分解的小问题能够与原问题具有相同的模式；
+ 原问题分解成子问题可以独立求解，子问题之间没有关联性（关联性是分治与动态规划的明显区别）；
+ 具有分解终止条件，即当问题足够小可以接求解；
+ 可以将子问题合并称为原问题，并且合并操作的复杂度不能太高，否则失去了分治的意义。

**应用1-求有序对与逆序对个数**

数组中可以通过有序度来表示一组数据的有序程度，用逆序对来表示数据无序的程度。

在求解有序对或逆序对个数时，最直接的方法是拿每个数字与后面的数字比较，观察有几个比它小。通过这种方式可以求出逆序对的个数。（时间复杂度为O(n*n)）

运用分治算法：将数组分为A和B两部分，分别计算A和B的逆序对个数K1和K2，然后再计算A和B之间的逆序对个数K3。数组A的逆序对个数为K1+K2+K3。其中K1和K2已经在子问题中得到求解，因此只需要考虑K3。对于K3利用归并过程中的辅助数组，可以简化求解的时间复杂度，因此分治可以整体降低时间复杂度。

![image-20210828165345487](https://gitee.com/tobing/imagebed/raw/master/image-20210828165345487.png)

**应用2-海量数据处理**

如果需要处理的数据量很大，没办法一次性放到内存中，这时常规的单机算法将无法完成运算任务。这时可以运用分治思想。将海量数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存中解决，然后在将小数据集合合并成大数据集合。

实际上利用这种分治的思想，不仅可以克服内存的限制，还可以里一股脑多线程或多机处理，加快处理速度。

### 回溯算法

典型应用：深度优先搜索、八皇后、0-1 背包问题、图的着色、旅行商问题、数独、全排列、正则表达式匹配

回溯的处理思想类似于枚举搜索。即枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，可以把问题的求解过程分为多个阶段。每个阶段会面对一个岔路口，这时先随意选择一条路走，当发现路走不通时，就会退到上一个岔路口，另选一种走法继续走。

**应用1-八皇后问题**

有一个 8x8 的棋盘，希望往里放 8 个棋子（皇后），每个棋子所在的行、列、对角线都不能有另一个棋子。八皇后问题就是期望找到所有满足这种要求的放棋子方式。

可以把这个问题划分成 8 个阶段，依次将 8 个棋子放到第一行、第二行、第三行……第八行。在放置的过程中，我们不停地检查当前的方法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种方法，继续尝试。

**应用2-0-1背包问题**

有一个背包，背包总的承载重量是 Wkg。现在我们有 n 个物品，每个物品的重量不等，并且不可分割。我们现在期望选择几件物品，装载到背包中。在不超过背包所能装载重量的前提下，如何让背包中物品的总重量最大？

求解：对于每个物品来说，都有两种选择，装进背包或者不装进背包。对于 n 个物品来说，总的装法就有 2^n 种，去掉总重量超过 Wkg 的，从剩下的装法中选择总重量最接近 Wkg的。

回溯法优化：为了不重复地穷举出这 2^n 种装法，可以把物品依次排列，整个问题就分解为了 n 个阶段，每个阶段对应一个物品怎么选择。先对第一个物品进行处理，选择装进去或者不装进去，然后再递归地处理剩下的物品。

**应用3-正则表达式**

判断一个给定的文本，能否跟给定的正则表达式匹配？（假设正则表达式中只包含「*」和「？」两种通配符）

求解：依次考察正则表达式中的每个字符，当是非通配符时，我们就直接跟文本的字符进行匹配，如果相同，则继续往下处理；如果不同，则回溯。

如果遇到特殊字符的时候，有多种处理方式。此时先随意选择一种匹配方案，然后继续考察剩下的字符。如果中途发现无法继续匹配下去，回到当前岔路口，重新选择另一种匹配方案，继续匹配。

### 动态规划







## 线性结构

### 数组

数组（Array）是一种**线性表**数据结构。它用一组**连续的内存空间**，来存储一组具有**相同类型的数据**。  

数组的**优点**是可以实现**随机访问**，这也是得益于数组的定义：连续的内存空间和相同的数据类型。

尽管数组根据下标定位数据的时间复杂度是O(1)，但是在数组中的一个元素的时间复杂度不是O(1)。即使是对排好序的数组，使用二分法查找数组元素其时间复杂度也是O(logN)；而对于乱序的查找查找，仍然需要遍历数组，时间复杂度为O(N)。

数组的**缺点**是其“插入”和“删除”操作比较低效。

为了保持数组的顺序性，在插入或删除时需要挪动大量元素，挪动大量元素的过程是非常低效的。

为了缓解数组“插入”和“删除”的低效，可以采用以下的方法。

+ 使用标记删除+批量删除代替立即删除，将需要删除的数据线标记，之后统一进行批量删除（类似于JVM标记清除算法）

+ 插入元素时，直接将被插入位置的元素放到最后(O(1))，不再挪动后面元素

很多的编程语言都提供了对数组的封装，如Java的ArrayList。这些容器将数组插入、删除数据的细节进行封装，并且支持动态扩容。但这并不代表着数组就一无是处了。

+ Java中容器，如ArrayList无法直接基础数据类型，储存int、long等基础数据类型时，需要Autoboxing、Unboxing，存在性能损坏
+ 如果数据规模预先已知，并且对数据的操作简单，可以直接使用数组
+ 在定义高维数组时，数组的表现方式比容器更加直观

### 链表

链表是一种线性表数据结构。它通过**指针**将一组**零散的内存块**（结点）串联在一起用于储存数据。链表主要有三种结构：单链表、双链表和循环链表。

**单链表**

单链表有两个特殊节点，把第一个节点称为**头节点**，把最后一个节点称为**尾节点**。

得益于链表的储存特性，在对单链表进行插入或删除时，只需要修改指针，而不再需要挪动大量元素。也正是因为这个特性，使得链表无法实现“随机访问”，如果要定位一个元素，需要从头节点进行遍历扫描。

**循环链表**

循环链表与单链表唯一不同是尾节点，单链表的尾节点next域是NULL，而循环链表的尾节点next是指向链表头节点。循环链表的优点是从链表的尾节点到头节点比较方便。

**双向链表**

双链表中每个节点具有两个指针，一个指向前驱节点，一个指向后继节点。尽管双链表需要额外的两个空间来储存前驱节点和后继节点，但是给链表操作带来了灵活性。Java中的LinkedList也是使用双链表来实现。

> 双向链表的好处。在删除单链表的一个元素的时候，需要操作当前元素的前一个元素，对于单链表，只能通过从头到尾进行遍历，时间复杂度为O(N)。此时如果使用双链表，只需要同 prev 指针获取到就可以获取到其前驱节点。

**链表编码技巧**

链表节点的改动涉及到指针/引用的改动，编程难点相对数组大很多。在对链表进行编码时，可以利用下面技巧来减少出现错误的可能。

+ 警惕指针丢失和内存泄漏  
+ 利用哨兵简化实现难度
+ 重点留意边界条件处理（链表为空/只包含一个节点/只包含两个节点/处理头和尾节点）
+ 举例画图，辅助思考

### 数组与链表

数组和链表都是线性数据结构，但是采用的却是截然不同的内存组织方式，这也导致了它们具有不同的特性。

| 对比      | 数组                 | 链表               |
| --------- | -------------------- | ------------------ |
| 插入/删除 | O(N)                 | O(1)               |
| 随机访问  | O(1)                 | O(N)               |
| CPU缓存   | 连续内存，可以利用   | 离散内存，无法利用 |
| 内存要求  | 要求连续内存         | 不要求连续内存     |
| 扩容      | 需要重新创建新数组   | 天然支持动态扩容   |
| 适用场景  | 内存使用苛刻，查询多 | 删除、插入多       |

树一种非线性结构，树的家族中，主要有二叉搜索树、平衡二叉树、红黑树以及递归树。

### 栈

栈是一种操作受限（**先进后出**）的**线性表**，只允许从一端插入和删除数据。

从功能上来看，数组和链表都可以替代栈。事实上，栈底层正是通过数组或链表的方式来实现，对数组或链表的操作进行限制就可以实现一个栈。

栈是一种基础的数据结构，使用场景很多。

**函数调用栈**

操作系统会为每个线程分配一块独立的内存空间，这块内存会被组织成为栈结构，用来储存函数调用时的临时变量。每进入一个函数，都会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。

> 事实上，并不一定非得使用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。

**表达式求值**

除了运用于函数调用栈，在编译器在进行表达式求值的时候，可以通过栈来实现。

使用栈实现表达式求值时，主要流程如下。（简化为只包含加减乘除的四则运算）

+ 编译器使用两个栈分别保存**操作数**和**运算符**
+ 将运算表达式从左到右遍历
  + 遇到数字，压入操作数栈
  + 遇到运算符，先与运算符栈栈顶元素进行比较
    + 比栈顶运算符优先级高，当前运算符压栈
    + 比栈顶运算符优先级低，取出栈顶运算符，从操作数栈的栈顶取出两个操作数运算，将结果压入操作数栈，继续比较

**括号匹配**

除此之外，栈还可以用于检测表达式中的括号是否匹配。

使用栈判断括号匹配时，主要流程如下。（简化为只包含三种括号，()/[]/{}）

+ s使用一个栈来保存未匹配的左括号，从左往右扫描字符串。
+ 扫描到左括号时，将其压栈；扫描到右括号时，将其与栈顶匹配（“(”与“)”、“[”与“]”、“{”与“}”）
+ 如果不匹配则表示不匹配直接返回false；如果匹配则将其出栈，并继续
+ 如果遍历完字符串，栈顶元素已经没有元素，则表示匹配成功
+ 如果遍历到右括号返现栈中没有元素，则表示不匹配

### 队列

与栈类似，队列也是一种操作受限（**先进先出**）的**线性表**，只允许从一端插入（队尾入队）和另一端删除数据（队头出队）。

#### 实现

**队列简单实现-数组**

在使用数组实现队列时，需要使用两个指针来分别指向队头(head)和队尾(tail)。随着出队和入队的不断进行，两个指针最终可以都会往数组尾部方向移动，直到队尾到达了数组尾部。

这时，尽管数组前方仍有很多空间，但是由于队尾到达了数组尾部(`tail==arr.length-1`)，无法新插入数据。因此，当插入元素时发现已经到达了数组尾部，可以将数据整体迁移到数组前端。

![datastruct_queue_data_move](https://gitee.com/tobing/imagebed/raw/master/datastruct_queue_data_move.png)

**循环队列**

使用数组实现队列，当tail==n时，会有数据搬移操作，只会导致入到入队操作受到影响。这时，可以采用循环队列的方式来避免数据搬移。

使用数组来实现循环队列时，我们将数组逻辑地看成一个环，首尾相连。数组中用tail和head来分别记录队尾和队首。随着对循环队列的出队和入队，tail和head不断往前移动，最终都会经过下标为length-1的节点。**由于已经将数组看成一个逻辑的环，当tail和head再次往前移动的时，就会到达下标为0节点。**（这时，tail和head向前移动优雅的处理方式是tail=(tail+1)%length）

使用循环队列时，当发现(tail+1)%length=head，即表示tail**追上**了head，即循环队列的空间已经使用完毕了。（为了区分循环队列为空的状态：tail==head，因此此处浪费了一个空间来区分队列满和空）

上面都是通过数组来实现一个队列，除此之外也可以通过链表来实现。

#### 应用

作为一种基础的数据结构，队列应用广泛，如高性能队列Disruptor、Linux环形缓存，都使用了循环并发队列；JUC并发包利用ArrayBlockingQueue来实现公平锁等。

**阻塞队列与并发队列**

**阻塞队列**就是在队列基础上添加了阻塞操作。简单而言，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。  

通过上述的定义，实际上实现了一个“生产者-消费者模型”。

基于阻塞队列实现“生产者-消费者模型”可以**有效地协调生产者和消费的速度**。当“生产者”生产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续“生产”。  

> Java中，使用你线程池的时候，线程池没有空闲线程时，新的任务请求线程资源时，一般有有两种处理策略。
>
> 第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。如果希望线程是能够公平地被处理，排队的线程可以通过队列来储存。
>
> 这种队列的是实现可以基于数组或链表来实现。
>
> 基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。  
>
> 基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的
> 请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。  （注意：循环队列的长度设定需要对并发数据有一定的预测，否则会丢失太多请求。）

**并发队列**就是保证多个线程可以同时安全地操作队列。

最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。

### 跳表

跳表是一种各方面性能都比较优秀的动态数据结构，可以支持快速的插入、删除、查找操作。

它编写简单，甚至可以替代红黑树。在删除、查找以及迭代输出有序序列这几个操作中，跳表与红黑树的时间复杂度一致。但是，在区间查询操作中，红黑树效率没有跳表高。对于区间查询操作，跳表可以做到O(logn)的时间复杂度定位区间的起点，然后再在原始的链表中往后遍历即可，非常高效。

除此之外，与红黑树相比，跳表更加容易理解，代码更加容易编写，结构灵活。因此在Redis中使用跳表而不是红黑树来实现有序集合。

**基于多级索引的链表**

链表加上多级索引的结构，就行是跳表。

在单链表中，即时是储存的数据有序，如果需要查找某个元素也需要从头到尾来遍历链表，主要一来查找效率就很低，时间复杂度为O(n)。

为了提高查找效率，对链表每两个节点提取一个节点到上一级，形成一级索引。对于一级索引，如果发现其元素仍然较多，可以在一级索引的基础上，每个两个节点提取一个节点到上一级，形成二级索引。以此类推，如果数据规模很大，可以形成多级的索引。

![image-20210810152214087](https://gitee.com/tobing/imagebed/raw/master/image-20210810152214087.png)

在建立了索引之后，如果要查找一个元素，就可以从最顶级索引开始，查找元素符合的区间。找到元素在顶层元素的区间之后，可以从区间左节点的down指针，往下一层开始，查找元素在下一次的区间，以此类推。

如果元素集合中有n个元素，假如索引有h级，最高级索引有2个节点。则n/(2^h)=2，解得h=log2(N)-1。如果包含元素链表一层，跳表的高度为log2(N)。

在跳表查询数据时，如果每层需要遍历m个节点（在每两个提前索引的场景中，m最大为3），跳表查询一个数据的时间复杂度为O(m\*logn)，即时间复杂度为O(logn)。

**空间复杂度**

由于每一层节点都是下面一层的两个节点合并提取而成，那么索引占用的空间为：n/2+n/4+n/8+...+2=n-2，因此跳表的空间复杂度为O(n)。

为了减少跳表索引占用的空间，可以每三个节点提取一个作为上面一层节点。这样一来，索引占用的空间为n/3+n/9+...+3=n/2，尽管时间复杂度仍然是O(n)，但是比上面一种方法减少了一般的存储空间。

实际开发中，索引仅仅储存关键key和几个指针，并不储存对象，对象数据仅在原始链表中储存，这是如果对象很大，则索引占用的数据几乎可以忽略。

**高效动态插入和删除**

在单链表中，插入和删除一个元素需要先定位到删除元素的位置/元素要插入的位置，再进行后续操作，这个过程是时间复杂度为O(n)。但是，对于跳表，定位过程是O(logn)，而后继操作的时间复杂度为O(1)，因此跳表中插入和删除的时间复杂度为O(logn)。

需要注意的是，删除操作是需要找到其前驱节点才能够删除，这时可以考虑使用双向链表。

**索引的动态更新**

在不断往跳表总插入数据时，如果不更新索引，可以会出现在某两个索引节点之间的数据非常多，极端情况下，跳表还会退化为单链表。因此作为一种动态数据结构，需要使用某种手段来维护索引与原始链表的大小之间的平衡，即保证如果链表节点多了，索引节点相应增加一点，避免复杂度退化，导致查找、插入、删除性能下降。

类似于红黑树、AVL树、平衡二叉树维护左右子树的平衡，跳表是通过随机函数维护索引节点的平衡性。即通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中。  

### 散列表

哈希表时数组的一种扩展，由数组演化而来，利用了数组支持下标随机访问数据的特性。

在数组中我们可以通过下标来随机访问数组中的一个数据。基于这一点，我们可以将要储存数据的某个属性值（key）转换为数组的下标，并将数据（value）存储到对应下标中。这样一来，我们就建立了这个属性值到数据的映射关系，并且可以实现随机访问。

在这个过程中，把**属性值（key）转换为数组下标**的映射方法叫做**散列函数/Hash函数/哈希函数**，而把散列函数计算得到的值叫做散列值/Hash值/哈希值。

总而言之，散列表利用数组支持以O(1)时间复杂度来实现下标随机访问的特性。通过散列函数把元素的键映射为下标，然后把数据存储到对应的下标。当按照键值查询元素时，使用同样的散列函数，将键值转化为数组下标，从对应数组下标位置取出数据。

#### 散列函数

在散列表的实现中，散列函数起着至关重要的作用。

散列函数是一种函数，可以定义为hash(key)，其中key表示元素的键值，hash(key)的值就是经过散列函数运算得到的散列值。散列函数的设计需要符合下面三点基本要求：

1. 散列函数计算得到的散列值是有个非负数（数组下标从0开始）

2. 如果key1=key2，那么hash(key1)=hash(kye2)（可重复性）

3. 如果key1≠key2，那么hash(key1)≠hash(key2)（实际无法实现）

#### 散列冲突

再好的散列函数也无法避免散列冲突，最常用解决哈希冲突的方案有两种：开放寻址法和链表法。

**开放寻址法**

出现散列冲突，重新探测一个空闲位置，将其插入。

最简单的探测方法是**线性探测**，即往散列表插入数据时，如果某个数据经过散列函数之后，储存位置已经被占用，从当前位置开始，依次从前往后查找，看是否有空闲的位置，如果直到尾部都没找到空闲的位置，会从表头开始，直到找到为止。

这时，如果需要**在散列表中查找元素**，就先通过散列函数求出查找元素对应的散列值，然后比较数组中小标为散列值的元素和要查找的元素。如果相等，说明是要找的元素；否则顺序往后依次查找。如果遍历**遇到数组中的空闲位置，还没找到，说明要查找元素不再散列表中。**

这时，如果需要实现**删除**，就**不能单纯把要删除的元素置空**。因为在查找的过程中，结束的判断条件是遇到一个空闲位置。如果删除时直接将元素之后，那么后面查找元素会出现逻辑错误。（有可能要查找的元素也出现了哈希冲突，而且是最终存放在删除元素之后）

因此如果需要实现删除，就可以将删除的元素使用标记删除（deleted）。当线性探测查找的时候，遇到标记为deleted的，不是停下来，而是继续往下探测。

可以发现，在使用线性探测法会存在很大问题。即当散列表中插入的数据越来越多时，散列冲突发生的概率会越来越大，空闲位置会越来越少，线性探测的时间会越来就，极端情况下可能需要探测整个散列表。同理，删除和查找也会出现同样的情况。

除了线性探测，开发寻址法还可以两种经典的探测方法：

+ 二次探测：线性探测的步长是n {n=1,2,3,4...}，二次探测步长时n^2 {n=1,2,3,4...}
+ 双重散列：不仅仅使用一个散列函数，而是使用一组散列函数，如果第一个散列得到的位置已经被占用，再用第二个，直到找到空闲位置

**链表法**

与开放寻址法相比，链表法更加常用，也要更加简单。

使用链表法时，在散列表中，每个“桶”或“槽”会对应一个链表，所有散列值相同的元素放在相同桶对应的链表中。

当插入的时候，先通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可,时间复杂度是 O(1)。  

当查找、删除一个元素，先通过散列函数计算对应散列槽位，然后遍历链表进行查找或删除。

查找和删除的时间复杂度与链表长度k成正比，理论上而言，k=n/m。（n为散列表数据个数，m为散列表桶的个数）

#### 如何选择冲突解决方法

哈希冲突主流有两种解决方法，开放寻址法和链表法。

在实际开发中，两种方法都比较常用

+ 开放寻址法（Java的ThreadLocalMap使用）
  + 所有数据保存在一个数组，冲突代价高，装载因子不能太小，浪费更多内存空间:x:
  + 删除数据麻烦，需要标记已删除数据:x:
  + 数据存储在数组，可以有效利用CPU缓存加速:heavy_check_mark:
  + 序列化简单:heavy_check_mark:

+ 链表法（Java的LinkedHashMap使用）
  + 数据保存在链表，无法利用CPU缓存，序列比较困难:x:
  + 链表法的链表节点需要猜创建，内存利用率较高:heavy_check_mark:
  + 更加灵活，可以改造为红黑树等高级动态数据结构，实现更加高效的查找:heavy_check_mark:

总而言之，当数据量较小，装载因子小的时候，使用开放寻址法；当数据量大，储存对象较大，适合使用链表法。

#### 负载因子

无论采用何种方式探测，当散列表空闲位置不足，哈希冲突发生的概率会大大提高。为了尽可能保证散列列表的操作效率，一般而言都会尽可能保证散列列表存在一定比例的空闲槽位。**通常会使用负载因子来表示空位的多少**。其公式为：

```
散列表的装载因子 = 填入表中的元素个数 / 散列表长度
```

装载因子越大，空闲位置越少，冲突越多，散列表性能会下降。

**动态扩容**

因此针对散列表，当装载因子过大，可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到新的散列表上。

需要注意在散列表数据迁移的过程中，不仅仅是散列表大小发送了变化，某些元素的储存位置也会发生变化，因此需要通过散列函数重新计算每个数据的储存位置。

类似于动态数组的数据迁移，散列表的数据迁移也可以均摊到每次操作中，时间复杂度是O(1)。

如果装载因子太多，会导致冲突太多；如果太小会导致内存浪费严重。当内存空间充足，可以降低负载因子阈值以提高执行效率；相反，如果内存空间紧张，对执行效率不高，可以增加负载因子的值。

#### 散列函数设计

散列表的查询效率并不能笼统说为O(1)，它与散列函数、装载因子、散列冲突有关。

极端情况下，恶意攻击者会通过精心构造数据，来使得所有数据经过散列函数之后，都散列到同一个槽。如果使用的是基于链表的冲突解决方法，散列表就会退化链表，时间复杂度骤增。最终有可能会导致DoS的目的。这就是**散列表碰撞攻击**的基本原理。

散列函数设计的好坏，决定了散列表冲突的概率大小，也直接决定了散列表的性能。好的散列表需要具备以下的特性：

+ 设计不能太复杂，太复杂的函数会消耗运算时间，间接影响散列表性能
+ 函数生成的值要尽可能随机并均匀分布，从而尽可能减少散列冲突概率，即使发生冲突，也可以均摊到每个桶中

#### 避免低效扩容

在散列表扩容时，需要先申请更大的内存，在进行数据迁移。如果一次性执行完全部的迁移过程是很慢的，甚至无法接受。

为此，可以将扩容的操作穿插在插入操作中，当装载因子达到阈值，先申请新空间，将数据迁移均摊到每次的新数据插入中，即每次除了将新元素插入到新散列表，还会从老散列表中拿出数据放到新的散列表。这个过程也被称为**渐进式rehash**。

在渐进rehash的过程中，对于查询操作，会分别到新、老散列表中查询。

在Redis中正是通过上述的方式来实现散列表的rehash过程。

#### HashMap in Java

Java 中的 HashMap是一种工业级散列表。

**初始大小**

HashMap 默认的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高HashMap 的性能。  

**装载因子和动态扩容**

最大装载因子默认是 0.75，当 HashMap 中元素个数超过 0.75*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。

**散列冲突方法**

最大装载因子默认是 0.75，当 HashMap 中元素个数超过 0.75*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。

在 JDK1.8 版本中，为了对 HashMap 做进一步优化，我们引入了红黑树。而当链表长度太长（默认超过 8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高 HashMap 的性能。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。

#### 散列表与链表

散列表与链表经常会被放在一起使用。如使用链表实现LRU缓存淘汰策略时，使用HashMap来快速定位节点位置；又如Redis中的有序集合时跳表（改进版链表）+散列表实现，又如Java中提供的LinkedHashMap容器，底层使用链表+散列表实现。

**LRU缓存淘汰算法**

LRU算法中，需要维护一个按访问时间从小到大有序排列的链表结构。由于缓冲大小有限，当缓存空间不够，需要淘汰一个数据时，直接将链表头部的节点删除。

当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放到链表的尾部；如果找到了，我们就把它移动到链表的尾部。因为查找数据需要遍历链表，所以单纯用链表实现的 LRU 缓存淘汰算法的时间复杂很高，是 O(n)。

在链表中添加、删除、查找一个数据都需要查找操作，时间复杂度为O(n)。这时将散列表和链表组合，可以将三个操作时间复杂度降为O(1)。

使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段 hnext。  这个字段用于散列表的链表法解决哈希冲突。

查找一个元素，先通过散列表快速找到元素，然后将其移动到双向链表尾部。

删除一个元素，先通过散列表快速扎到元素，然后将其从链表中删除。

添加一个数据，先通过散列表判断是否存在于缓存。存在则将其移动尾部；不存在则判断缓存是否已满；满则将双向链表头节点删除，然后将数据放到链表尾部；不满则直接放到尾部。

**Redis 有序集合**

有序集合中，每个成员对象有两个重要的属性，key（键值）和score（分值）。在有序集合中，不仅通过score查找数据，还通过key查找数据。如果仅按照score来组织成跳表，那么通过key来查找的时候效率就非常低，反之亦是如此。于是可以考虑先按照score来将数据组织成跳表，然后按照剑指映射构建一个散列表，可以实现通过key来快速查找到指定元素。

**Java LinkedHashMap**

LinkedHashMap通过散列表和链表组合，处理可以实现类似于HashMap的快速查找功能，还可以实现记录元素插入/访问的顺序。（可以在构造函数中指定释放按照访问时间来排序）

**总结**

散列表这种数据结构虽然支持高效的插入、删除、查找操作，但是散列表中的数据是通过散列函数打散无规律储存的。即它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，需要将散列表中数据拷贝到数组中再排序。

但是散列表是动态数据结构，不停地有数据插入和删除，当希望按照顺序遍历散列表中的数据的时候，都需要先排序，这样一来，效率就很低。为了解决这个问题，通常会将散列表和链表一起使用。

### 



## 树

树一种非线性结构，树的家族中，主要有二叉搜索树、平衡二叉树、红黑树以及递归树。

在一棵树中，一个节点可以有多个子节点，一个子节点仅能有一个父节点，同一个父节点的若干节点之间互称为**兄弟节点**。把没有父节点的节点叫做**根节点**，把没有子节点的节点叫做**叶子节点**。

在树中，有几个比较相似的概念：

+ 节点的高度：节点到叶子节点的最长路径（从下往上度量）
+ 节点深度：根节点到这个节点记录的边的个数（从上往下的度量）
+ 节点的层：节点的深度+1
+ 树的高度：根节点的高度

### 二叉树 

二叉树(Binary Tree)中，每个节点最多有两个子节点，分别是**左子节点**和**右子节点**。二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。

二叉树中，有两种比较特殊的二叉树：**满二叉树**和**完全二叉树**

+ 满二叉树：叶子节点全部都在底层，除了叶子节点之外，每个节点都有左右两个子节点
+ 完全二叉树：叶子节点在最底下两层，最后一层的叶子节点都靠走排列，并且处理最后一层，其他层的节点个数都要达到最大。

满二叉树特征明显， 因此会通过一个单独的概念来描述。相比之下，完全二叉树的特征就显得没有这么明显。之所以单独使用完全二叉树这个概念来单独描述一种二叉树，是因为这种二叉树比较特殊的特性，涉及到二叉树的储存。

#### 二叉树储存

二叉树的储存可以有两种方式，一种是基于指针或引用的**二叉链式储存法**，一种是基于数组的**顺序存储法**。

**链式存储法**简单、直观。每个节点有三个字段，其中一个储存数据，另外两个是指向左右子节点的指针。通过根节点可以通过左右子节点的指针把整棵树串起来。

```java
class Node {
    private Object e;
    private Node left, right;
}
```

**顺序存储法**基于数组实现。它把根节点储存到下标为` i=1 `的位置，左节点储存到下标为` 2*i=2 `的位置，右节点储存到下标为 `2*i+1=3 `的位置。以此类推，左节点的左子节点储存在`2*i=2*2=4` 的位置，右节点储存在`2*i+1=2*2+1=5`的位置。

在使用顺序存储法的时候，如果储存的一个树是完全二叉树，那么它**浪费**的空间仅仅是下标为0的储存位置。如果是非完全二叉树，将会浪费比较多的数组存储空间。

综上所述，如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要**存储额外的左右子节点的指针**。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。  

#### 二叉树遍历

二叉树的经典遍历算法有三种：**前序遍历**，**中序遍历**和**后序遍历**。

+ 前序遍历：对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。
+ 中序遍历：对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。
+ 后序遍历：对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。  

实际上，二叉树的前、中、后序遍历就是一个递归的过程。比如，前序遍历，其实就是先打印根节点，然后再递归地打印左子树，最后递归地打印右子树。  

在前、中、后序遍历中，每个节点最多被访问两次，因此遍历操作的时间复杂，跟根节点个树n成正比，即二叉树遍历的时间复杂度为O(n)。

> 前序遍历，中序遍历，后序遍历的输出序列特点，[参考博客](https://www.cnblogs.com/jiaxin359/p/9512348.html)

```java
——----------------
|根节点|左子树|右子树|
------------------
——----------------
|左子树|根节点|右子树|
------------------
——----------------
|左子树|右子树|根节点|
------------------ 
```

### 二叉查找树

二叉查找树(Binary Search Tree)是二叉树中最常用的一种类型，也叫二叉搜索树,是为了实现**快速查找**而生的。但是，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。

**二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。**  

#### BST 查找操作

在二叉查找树中查找一个节点。

先取根节点，如果它等于我们要查找的数据，那就返回。

如果要查找的数据比根节点的值小，那就在左子树中递归查找；

如果要查找的数据比根节点的值大，那就在右子树中递归查找。  

#### BST 插入操作

插入过程类似于查找过程。新插入的节点一般都是在叶子节点上，在插入时只需要从根节点，依次比较要插入的数据和节点的大小关系。
如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点
的位置；如果不为空，就再递归遍历右子树，查找插入位置。

同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。  

#### BST 删除操作

<font style="color:red">**删除操作比较复杂。**</font>针对要删除节点的子节点个数不同，需要分三种情况处理。

+ 删除的节点没有子节点，直接将父节点中，指向删除节点的指针置为null即可；
+ 删除的节点的节点只有一个子节点（左/右），只需要更新父节点中，指向要删除节点的指针，让它指向删除节点的子节点即可；
+ 删除的节点的节点有两个子节点，这种情况比较复复杂。需要先找到这个节点的最小节点，把它替换到删除的节点。再删除这个最小节点，因为最小节点肯定没有左子节点（如果有就不是最小子节点了），因此，可以应用上述的两条规则来删除这个最小节点。

关于BST的输出操作，还有一个比较简单、取巧的方法，就是单纯将要删除的节点**标记**为“已删除”，但并不真正从树中将这个节点去掉。这样一来就不需要执行上述的操作，同时这种方式也没有增加插入、查找操作代码实现的难度。这种方式的唯一缺点就是比较浪费内存空间。

#### BST 其他操作

除了插入、删除、查找，BST还支持**快速地查找最大节点和最小节点、前驱节点和后继节点**。  除此之外，BST还有一个重要的特性，即

<font style="color:red">**中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效。因此，二叉查找树也叫作二叉排序树。**  </font>

#### BST 支持重复数据

在实际的软件开发中，BST储存的是包含很多字段的对象。利用对象的某个字段作为键值(key)来构建二叉树。这种情况下，把对象汇总其他字段叫做**卫星数据**。

为了能够储存重复数据，可以有两种解决方法。

+ 第一种比较简单。二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和
  支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。

+ 第二种方法比较不好理解，不过更加优雅。  每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。  

  这种情况下，当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。

  对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。  

#### BST 时间复杂度分析

实际上，二叉查找树的形态各式各样。它们的查找、插入、删除操作的执行效率都是不一样的。有的叉查找树，根节点的左右子树极度不平衡，退化成了链表，所以查找的时间复杂度就变成了O(n)。  对于完全二叉树、满二叉树，他们的查找、插入、删除操作也会发生相应的变化。

从上面来看，<font style="color:red">**不管操作是插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是 O(height)。**  </font>这样一来，就**可以把求时间复杂度问题，转换为求一颗包含n个节点的完全二叉树的高度。**

#### BST 存在意义

散列表的插入、删除、查找操作的时间复杂度可以做到常量级的O(1)，非常高效。而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 O(logn)  ，相对散列表，好像并没有什么优势，那我们为什么还要用二叉查找树呢？  

主要有下面几个原因：

+ 散列表的数据时无序储存的，如果要输出有序数据，需要先排序。对于BST，只需要中序遍历可以**在O(n)时间复杂度输出有序的数据序列**。
+ 散列表扩容耗时多，且当遇到散列冲突时，性能不稳定，尽管BST不稳定，但是工程中，**最常用的平衡BST性能非常稳定，时间复杂度稳定在O(logN)**。
+ 笼统来讲，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logN 小，所以实际的查找速度可能不一定比 O(logN) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。
+ **散列表的构造比二叉查找树要复杂，需要考虑的东西很多。**比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。  
+ 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然**会浪费一定的存储空间**。  

#### BST 代码实现

+ [插入](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L65)
+ [包含](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L91)
+ [前序遍历【递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L118)
+ [中序遍历【递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L140)
+ [后序遍历【递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L163)
+ [前序遍历【非递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L186)
+ [中序遍历【非递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L218)
+ [后序遍历【非递归】](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L212)
+ [层序遍历 ](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L224)
+ [获取最大值](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L260)
+ [获取最小值](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L249)
+ [删除最大值](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L316)
+ [删除最小值](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L224)
+ [删除任意元素](https://github.com/Tobingindex/java-code/blob/master/java-datastruct-algorithm/src/main/java/top/tobing/datastruct/tree/binary_search_tree/BST.java#L342)

### AVL

### 2-3 树

2-3树是一种绝对平衡的树。无论什么时候，在2-3树中从根节点到所有叶子节点的高度一致。

红黑树背后的逻辑与2-3树密切相关，通过2-3树可以更加简单理解红黑树。、

#### 三种节点

2-3树中有三种节点，其中有两种是稳定状态的节点，另外一种是临时状态的节点。

+ 2-节点：2个孩子，与普通的二分查找树节点定义类似【稳定状态】

+ 3-节点：3个孩子，一个node储存两个值和3个孩子指针【稳定状态】

+ 4-节点：4个孩子，一个node储存三个值和4个孩子指针【临时状态】

> 其中，4节点作为临时状态，便于状态之间的转换，稳定的2-3树不会存在节点。

![two-three-tree-node-type](https://gitee.com/tobing/imagebed/raw/master/two-three-tree-node-type.png)

#### 插入操作

由于存在不同类型的节点，因此往2-3树中添加一个节点存在多种情况。

**往2节点中插入一个元素**

![image-20210812122037293](https://gitee.com/tobing/imagebed/raw/master/image-20210812122037293.png)

> 2节点会变成一个3节点

**往3节点中插入一个元素（3节点为根节点）**

![image-20210812122046966](https://gitee.com/tobing/imagebed/raw/master/image-20210812122046966.png)

> 3节点先会临时变成一个4节点，然后4节点向上分裂，形成3个2节点。

**往3节点中插入一个元素（3节点为叶子节点，父节点为2-节点）**

![image-20210812122146217](https://gitee.com/tobing/imagebed/raw/master/image-20210812122146217.png)

> 往叶子节点的3节点插入一个节点，该3节点先会变成4节点，然后向上分裂，形成3个2节点，但为了保证平衡，形成的3个2节点的中间节点会向上合并到父节点，此时由于父节点是一个2节点，合并形成一个3节点。

**往3节点中插入一个元素（3节点为叶子节点，父节点为3-节点）**

![image-20210812122334796](https://gitee.com/tobing/imagebed/raw/master/image-20210812122334796.png)

> 前面部分和上面类似，分裂形成的3个2节点中，中间的2节点会向上合并到父节点，但此时由于父节点已经是3节点，合并会形成一个4节点，4节点会向上分裂。

### 红黑树

红黑树是相对平衡的二叉树，在平衡度上比不上AVL树的高度平衡。因此在查询效率上，红黑树比不上AVL树。但是AVL为了保持这种高度平衡，需要在插入或删除元素的时候，去花费较多的资源来维护高度平衡。对于红黑树，维护去相对平衡所消耗的资源相对较少，因此相应插入、删除、查找的性能更为稳定，也正是因此，红黑树的使用场景非常丰富，在Java的TreeMap就是使用红黑树来实现。

在上面提到2-3树中，会存在2节点或3节点，而在二叉树中，只会存在2节点。于是有人就提出使用二叉树的方式来表示2-3树这种非二叉树。那就是红黑树。

#### 红黑树与2-3树

由于在2-3树中，只会存在2节点或3节点。对于2节点是符合二叉树的定义的，因此我们只需通过一种特殊的方式来表示3节点即可。在讲[2-3树节点类型](# 三种节点)的时候曾经提到，三节点的左边的值是小于右边的值，那是否可以把左边的值当做右边的一个左孩子呢？如下图所示。

![image-20210812130503695](https://gitee.com/tobing/imagebed/raw/master/image-20210812130503695.png)

> 非得把b看做c的左孩子吗？把c看做b的右孩子不行吗？
>
> 不一定非得把b看做c的左孩子，选b还是选c作为父节点，源于对红黑树的定义。把b作为父节点，把整棵树称为右倾的，把c作为父节点，把整棵树称为左倾的。下面都是基于左倾来定义的。

上面我们可以通过两个2节点来表示2-3树里面的一个3节点。那么怎么区分一个节点的左孩子是**正常的左孩子**还是由**2-3树3节点拆分得到的左孩子**呢？

答案已经在上面的图中展示， 就将一个节点定义为红色代表该节点是由**2-3树3节点拆分得到的左孩子**。

下面是通过一颗红黑树来表示2-3树，这里可以自己画一下下面2-3树对应的红黑树，自己加深理解。

![image-20210812131323666](https://gitee.com/tobing/imagebed/raw/master/image-20210812131323666.png)

#### 红黑树定义解析

上面提到了红黑树的由来以及与2-3树之间的转换关系，下面是《算法导论》对于一颗红黑树的定义。

+ 每个节点或者是红色，或者是黑色
+ 根节点是黑色的
+ 每个叶子节点（最后的空节点）是黑色的
+ 如果一个节点是红色的，那么它的孩子节点都是黑色的
+ 任意一个节点到叶子节点，经过的黑色节点是一样的

由于红黑树源于2-3树，下面通过2-3树来解释红黑树的定义背后蕴含的原因。

**「每个节点或者是红色，或者是黑色」**

这个不难理解，由于2-3树只有2节点或3节点。对于2节点可以直接采用普通的二叉树节点表示；对于3节点需要拆分为两个2节点表示，其中左孩子用红色来特别标识，与其他普通左右孩子区分。因此一颗红黑树中，要么就是红色节点，要么是非红色节点（黑色）。

**「根节点是黑色的」**

2-3树中根节点要么是2节点，要么是3节点，在红黑树中，2节点和3节点的表示中，其最顶层节点都是黑色的，因此根节点必然为黑色。

**「每个叶子节点（最后的空节点）是黑色的」**

这个是定义，无须纠结。

**「如果一个节点是红色的，那么它的孩子节点都是黑色的」**

一个节点是红色，表示是2-3树中的3节点的左边值，对于3节点，其左右孩子只能也是2节点或3节点，2节点必然是黑色，3节点的在红黑树的表示中，其上面的节点也是黑色。因此红色节点的孩子节点必然是黑色。（可以结合「根节点是黑色」理解）

**「任意一个节点到叶子节点，经过的黑色节点是一样的」**

2-3树是一种绝对平衡的二叉树，因此其根节点到所有叶子节点是一样的。在红黑树中是通过两个2节点来表示3节点的，理解如下图。

![image-20210812133428105](https://gitee.com/tobing/imagebed/raw/master/image-20210812133428105.png)

#### 红黑树-添加元素实现

我们仍然基于2-3添加元素过程来描述红黑树添加元素的过程。对于2-3树，添加的元素永远是先合并到一个节点中去。

+ 添加进一个2节点，形成3节点
+ 添加进一个3节点，形成临时4节点

因此我们说，如果先不考虑后继的4-节点分解，一个节点添加进去红黑树永远先是红色。

**添加进一个2-节点**

对于2-3树，将一个值(<font style="color:red">a</font>)添加进去2节点(b)，可以存在两种情况。

+ a<b，形成3节点摆放方式为(<font style="color:red">a</font>，b)。对于红黑树，即放到节点的左节点，符合红黑树定义，无需修改。
+ a>b，形成3节点摆放方式为(b，<font style="color:red">a</font>)。对于红黑树，即方到节点的右节点，不符合红黑树定义，需要将节点进行左旋转。

> 左旋转

对于红黑树，左旋转前后，a(=37)与b(=42)的父子关系发生改变，颜色也会发生改变。

![image-20210812145449616](https://gitee.com/tobing/imagebed/raw/master/image-20210812145449616.png)

**添加进一个3-节点**

对于2-3树，将一个值(<font style="color:red">a</font>)添加进去3节点(<font style="color:red">b</font>，c)，可以存着以下情况。

+ <font style="color:red">b</font><c<<font style="color:red">a</font>，形成临时4节点为(<font style="color:red">b</font>，c，<font style="color:red">a</font>)。对于红黑树，根为c，分裂之后，不符合红黑树定义，需要进行**颜色翻转**。
+ <font style="color:red">a</font><<font style="color:red">b</font><c，形成临时4节点为(<font style="color:red">a</font>，<font style="color:red">b</font>，c)。对于红黑树，根为c，分裂之后，不符合红黑树定义，需要进行**右旋转+颜色翻转**。
+ <font style="color:red">b</font><<font style="color:red">a</font><c，形成临时4节点为(<font style="color:red">b</font>，<font style="color:red">a</font>，c)。对于红黑树，根为c，插入之后，不符合红黑树定义，需要进行**左旋转+右旋转+颜色翻转**。

> 颜色翻转

![image-20210812152025262](https://gitee.com/tobing/imagebed/raw/master/image-20210812152025262.png)

> 右旋转+颜色翻转，以a<b<c为例。

![image-20210812154131852](https://gitee.com/tobing/imagebed/raw/master/image-20210812154131852.png)

> 先左旋转，转换为上面情况

![image-20210812160908691](https://gitee.com/tobing/imagebed/raw/master/image-20210812160908691.png)

**元素添加总结**

综上所述，对于添加一个元素a，主要存在下面情况。

+ 元素添加进一个2节点(b)：
  + a<b：无需处理
  + a>b：左旋转
+ 元素添加进一个3节点(b，c)：
  + b<c<a：颜色翻转
  + a<b<c：右旋转+颜色翻转
  + b<a<c：左旋转+右旋转+颜色翻转

![image-20210812160230348](https://gitee.com/tobing/imagebed/raw/master/image-20210812160230348.png)



### 堆

堆是一种特殊的树，需要满足以下两点。

+ 堆是一个完全二叉树；
+ 堆中每个节点值都必须大于等于(或小于等于)其子树中每个节点的值。

> 如果每个节点值大于等于子树每个节点值，称为「大顶堆」。
>
> 如果每个节点值小于等于子树每个节点值，称为「小顶堆」。

树的储存主要有两种方式，指针和数组。由于堆是完全二叉树，因此适合使用数组进行储存。使用数组来储存堆，可以无需储存左右节点的指针，节省内存空间，可以简单同数组下标来找到一个节点的左右子节点和父节点。

对于数组中下标为`i`的节点，其左子节点下标为`i*2`，右子节点下标为`i*2+1`，父节点下标为`i/2`。

#### Heap 堆化

对于往堆中添加或删除一个元素，如果不采取特定措施，就可能会导致堆的特性被破坏。如

+ 对于添加操作，如果直接把元素添加到数组末尾，可能会破坏堆的大小特性
+ 对于删除操作，如果直接把要删除元素的位置清除，可能会破坏堆的形状特性

为了保证新插入或删除一个元素之后，仍然保持堆的特性，需要在执行这两个操作之后，对堆进行重新调整，使其重新满足对的特性，这个过程称为「堆化」。

堆化主要有两种实现方式，分别是**自下而上**和**从上往下**。

+ 对于往堆中添加一个元素，使用自下而上的方式；
+ 对于从堆中删除堆顶元素，使用从上往下的方式。

**往堆中添加一个元素**

往堆中添加元素时，先直接把元素放到数组末尾。此时由于新添加元素可能会不满足对的定义，即元素可能会比父节点大（对于最大堆而言）。为了保持对的大小特性，新添加的元素需要依次与父节点比较大小，如果比父节点大，则与父节点进行交换，交换之后还是执行同样的操作，直到符合堆的定义位置。

这种从子节点到父节点的堆化过程，就是「自下而上」的堆化过程。

**从堆中删除堆顶元素**

对于最大堆，其堆顶元素为堆中最大元素；对于最小堆，其堆顶元素为堆中最小元素。

对于从堆中删除堆顶元素，先把堆顶元素删除，然后把数组末尾元素补充到堆顶元素，然后将堆顶元素依次与子节点比较，如果元素比子节点小（对于最大堆而言），则交换两者位置。交换之后仍然重复同样操作，直到符合堆的定义为止。

> 如果删除堆顶元素直接将堆顶元素删除，然后从子节点中分别选出孩子节点进行替换，很大概率会出现数组空洞，因此最好的办法还是先把末尾元素替换。

对于堆而言，树的高度不会超过log2(N)，而堆化的过程实际上就是顺着节点所在路径进行比较交换，因此堆化的时间复杂度与树的高度成正比，为O(logN)。而插入和删除堆顶元素的主要逻辑就是堆化，因此它们的时间复杂度都为O(logN)。

#### Heap 堆排序

借助堆来实现排序算法，称为堆排序。这种排序算法的时间复杂度非常未定，为O(n*logN)，是一直用原地排序算法。

堆排序的过程大致可以分解为两个大步骤，**建堆**和**排序**。

**建堆**

对于建堆过程，为了保证为原地性，执行可具有两种建堆过程。

+ 自下而上：尽管数组包含n个元素，但假设起初堆中只有一个元素，即下标为1的元素。之后将下标从2~n的数据依次插入到堆中(先放到数组尾部，再依次与父节点比较)，这样就将包含n个数据的数组组织为一个堆。【从前往后】
+ 自上而下：从后往前处理数组，对于每个元素，依次向下堆化，与其左右子节点比较交换。

在使用自上而下的方式时，由于叶子节点只能与自己比较，因此比较过程只需从非叶子节点开始。对于完全二叉树，叶子节点下标是`n/2+1`到`n`。因此我们只需要对`n/2`前面的节点进行堆化即可。

由于叶子节点无需堆化，因此堆化的节点数从倒数第二层开始。每个节点的堆化过程需要比较交换的节点个数与节点高度k成正比。

假设树的总高度为h，那么建堆的时间复杂度为：

```java
// 假设总高度为h
S=sum(第i层节点数*第i节点堆化花费的时间) { i∈(0, h-1) }
// 代入数据 
S=(2^0)*h+(2^1)*(h-1)+(2^2)*(h-2)+...+(2^(h-1))*1
// 化简可得
S=2^(h+1)-h-2
// 代入h=log2(N)
S=O(n)
```

综上所述，建堆过程的时间复杂度为O(n)。

**排序**

建堆完成之后，数组中的数据按照大顶堆特性组织。

我们只需参考删除堆顶元素，依次将堆顶元素进行删除（与数组末尾元素交换，重新堆化），即可。

![image-20210813110355123](https://gitee.com/tobing/imagebed/raw/master/image-20210813110355123.png)

经过建堆与排序两个过程，就可以将数组中的排行了序。

**堆排序的分析**

在堆排序的过程中，堆排序是基于原数组进行排序，是**原地**排序算法。

堆排序包括建堆和排序两个操作，建堆的时间复杂度为O(n)，排序过程的时间复杂度为O(n\*logN)，因此整体时间复杂度为O(n\*logN)。

在排序过程中，存在将堆的最后一个节点和堆顶节点交换，有可能会改变值相同数据的原始相对顺序，因此堆排序是不稳定的排序算法。

#### 堆排与快排

尽管堆排和快排的时间复杂度都是O(n\*logN)，甚至堆排的时间复杂度更加稳定。但是，堆排的性能却没有快排性能好，主要有两方面的原因。

+ **堆排的数据访问方式没有快排优化**。快排顺序访问，堆排跳着访问，对CPU缓存不友好。
+ **对于同样数据，排序过程中，堆排的数据交换次数要多于快排**。

#### Heap 应用

堆有几个典型的应用：优先级队列、Top K问题、求中位数。

**优先队列**

优先队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的最先出队。通过堆来实现优先级是最直接、最高效的。堆和优先队列极为相似，很一个堆可以看做一个优先队列。往优先队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。

优先队列使用场景广泛，很多数据结构和算法都依赖它。不仅如此，很多语言都提供了优先级队列的实现，如Java的PriorityQueue。

> 优先队列应用1-合并有序小文件

场景：有100个小文件，每个100MB，每个文件储存有序的字符串，要将100个小文件合并成一个有序的大文件。

思路：从100个小文件中，各取一个字符串，一个优先队列中，取出栈顶元素(最小/大的字符串)放到合并后的大文件中，然后将其从数组中删除。循环上面的过后才能，知道100个小文件中的数据都放到大文件中。

> 优先队列应用2-高性能定时器

场景：定时器需要维护多个定时任务，每个任务都有一个触发执行的时间点，如果时间到达，定时器需要调度相应任务执行。

思路：最简单的方法就是每个单位时间，循环遍历一遍任务，看任务是否到达设定的执行时间，到达则执行。这种方式执行简单，但也比较低效。一方面，如果任务里约定的执行时间很长，如果每次都要扫描会浪费时间；另一方面，如果任务列表很大，每次都去扫描会比较耗时。

优先队列解决：按照任务设定的执行时间，将任务储存在优先队列中，队首(堆顶)储存的是最优先执行的任务。这样一来，定时器只需拿到队首的执行时间，与当前时间点相见，就可以得到一个时间间隔T，在T秒之后执行任务即可。当执行完一个任务，再次计算堆顶任务执行时间与当前时间的间隔T2，定时器可以在等待T2之后在执行，可以避免轮询。

**Top K问题**

Top K问题可以划分为两类子问题。

+ 静态数据结构Top K问题，即数据事先确定，不会改变；
+ 动态数据结构Top K问题，数据事先不确定，数据动态添加到集合。

对于静态的Top K问题，只需要维护大小为K的**小顶堆**。顺序遍历数组，数组中取出元素与堆顶比较。比堆顶元素大则将堆顶元素移除，插入该元素；如果比堆顶元素小，则不做处理，继续遍历数组。这样一来，等数组中元素遍历完毕之后，堆中的数据就是前K大数据。（时间复杂度O(n*logK)）

对于动态的Top K问题，主要有两个操作，一个是添加数据，另外一个是获取Top K数据。对于Top K问题，主要有两种策略。

+ 每次添加数据时，维护小顶堆，等需要Top K数据可以直接拿（均摊法）
+ 每次添加数据，不维护小顶堆，等需要查询再遍历数组，建立小顶堆

**中位数问题**

在有序数组中，如果数组元素个数奇数时，中位数是第`n/2+1`个数据；如果个数是偶数时，是第`n/2`和第`n/2+1`个元素的平均数。

对于一组静态数据，中位数相对固定，可以先进行排序，再按照上面规则取即可，效率可行；

对于一组动态数据，随着元素的改变，中位数一直变化，如果每次都要进行排序，效率就太低了。

为了应对动态数据的中位数需求，可以维护两个堆。

+ 一个大顶堆，储存前半部分的数据，堆顶元素为`[0~n/2]`的最大值
+ 一个小顶堆，储存后半部分的数据，栈顶元素为`[n/2+1~n]`的最小值

当n为奇数，大顶堆储存n/2+1个元素，并且中位数为大顶堆堆顶元素；

当n为偶数，大顶堆储存n/2个元素，小顶堆储存n/2个元素，两者堆顶的元素平均值就是中位数。

### Trie树

### 并查集

并查集是一种特殊的树形结构，常用于解决连接问题。如判断网络中节点的连接状态，数学中的集合类的实现。由于连接问题（并查集）比路径问题（堆）回答的要少，因此使用并查集来判断集合中的节点是否连接花费的消耗要更低。

并查集中，对于一组数据主要包含两个操作：

+ `union(p, q)`：将p和q两个节点所在的集合进行合并
+ `isConnected(p, q)`：判断p和q两个节点是否连接

并查集的主要有以下几种实现方式：

+ Quick Find：通过数组实现。
+ Quick Union：通过ADT树来实现。

实际使用中Quick Union，由于简单的Quick Union实现中性能较差，下面是对于Quick Union的优化：

+ 考虑size：在union操作时，根据集合p和集合q的元素个数，将元素个数少的合并到元素个数多的地方；
+ 考虑rank：考虑size的方式，可能会出现将层数高的合并到层数低的集合，导致合并的层数更高，于是引入rank，根据层数来决定合并；
+ 路径压缩：路径压缩在每次find操作时可以不但减少集合的层数，能够有效提供性能。

![image-20210831154528907](https://gitee.com/tobing/imagebed/raw/master/image-20210831154528907.png)



## 图

### 基本概念

图是一种比树更加复杂的非线性结构。在图这种数据结构中，主要有以下定义。

+ 顶点(Vertex)：图中的每个元素都叫做顶点
+ 边(edge)：图中一个顶点与其他任意顶点的关系叫做边
+ 度(degree)：跟顶点相连接的边的条数

图根据边的「方向」可以分为：

+ 有向图：边有方向。根据边的方向，对一个顶点，度可以分为「入度」和「出度」
  + 入度：表示有多少条边指向该顶点
  + 出度：表示有多少条边是以这个顶点为起点指向其他顶点的
+ 无向图：边无方向。通过「度」来表示一个顶点有多少条边。

根据边的重要程度，可以将图分为：

+ 带权图：每条边有一个权重
+ 无权图：边无权重

### 表示方式

图主要有两种表示方式，分别是**邻接矩阵**和**邻接表**。

**邻接矩阵**

通过一个二维数组来储存图的关系。

+ 对于无向图，如果`顶点i`与`顶点j`之间有关系，将`A[i][j]`和`A[j][i]`标记为1；

+ 对于有向图，如果`顶点i`到`顶点j`方向有关系，将`A[i][j]`标记为1，同理`顶点j`到`顶点i`方向上有关系，将`A[j][i]`标记为1。

+ 对于带权图，数组中储存的就是相应的权重。

使用邻接矩阵来表达图，简单、直观，由于基于数组，获取两个顶点之间的关系非常高效。再者通过邻接矩阵来储存图，可以将很多图的运算转换为对矩阵之间的运算。但是邻接矩阵比较浪费内存。

+ 对于无向图，`A[i][j]`与`A[j][i]`存在重复表示
+ 对于稀疏图，图上顶点很多，但是每个顶点的边不多，使用邻接矩阵会浪费大量空间

**邻接表**

邻接表的表示方式类似于散列表中使用链地址法来处理依赖冲突的问题。

邻接表的每个顶点对一个一条链表，链表中储存的是与顶点相连接的其他顶点。

> 和散列表类似，如果一个顶点与较多的顶点存在连接关系，那么该顶点连接的链表可能会过长，从而导致在上面的搜索效率较低，这时可以将链表改造为红黑树等查找效率更为高效的数据结构。

![image-20210814154858696](https://gitee.com/tobing/imagebed/raw/master/image-20210814154858696.png)

**邻接表与邻接矩阵**

对于储存微信/微博这些大用户量的好友关系，并不适合使用邻接矩阵。因为对于这些场景，用户很多(顶点)，但是用户之间的好友关系(边)确实少量的，如果使用临界矩阵，将会存在大量浪费。于是我们可以使用邻接表来储存好友关系。

但是使用邻接表来储存好友之间的关系，对于判断用户之间关注与被关注的关系时，查找效率较低。

+ 对于判断A是否关注B，需要遍历A的链表，查看是否存在B；
+ 对于判断A的粉丝列表，则需要遍历所有节点的所有链表，找出链表存在A的顶点。

显然，上述的效率是非常低的。

+ 对于查找A的粉丝，可以创建另外一张邻接表，节点的链表上储存的是每个用户的粉丝列表
+ 对于在节点单链表查询效率低，可以使用高级动态数据结构，如跳表、红黑树等

如通过跳表，按照用户名称首字母排序，储存用户的粉丝数据。

对于少量用户，用户的关系信息可以全部放到内存中；但是对于如微博微信这种存在海量用户数据的情况，单节点无法储存这些用户的关系信息。这时候可以借助哈希映射来将用户的关系数据进行分配映射到不同的机器中。

### 图的遍历

对于图的搜索，主流有两种方法：深度优先搜索算法和广度优先搜索算法。

#### 广度优先算法(BFS)

采用层层推进的搜索策略，即先查找里起始顶点最近的，然后是次近的，依次往外搜索。

BFS思想简单直观、但是实现起来并不容易，需要使用到一些辅助变量。

+ visited：**记录已经被访问的顶点**，避免被重复访问。如果q被范围，则visited[q]=true；
+ queue：一个队列，用于**储存已经被访问、但相连的顶点还没被访问的顶点。**广度优先遍历是逐层访问的，即把第k层顶点访问完成之后，才能访问k+1层。因此在访问第k层顶点时，需要把第k层顶点记录下来，稍后通过第k层找第k+1层顶点。
+ prev：**记录搜索路径。**从顶点s开始，广度优先搜索到顶点t之后，prev数组储存的是搜索的路径。

![image-20210814162952305](https://gitee.com/tobing/imagebed/raw/master/image-20210814162952305.png)

**时间复杂度**：O(V+E)，V为顶点个数，E为边的个数。对于一个连通图，所有顶点都是连通的，E大于等于V-1，因此时间复杂度可以简写为O(E)。

**空间复杂度**：O(V)，BFS需要三个辅助变量 visited、queue、prev，三个储存空间大小都不会超过顶点个数。

#### 深度优先算法(DFS)

采用走迷宫的方式，在入口中(顶点)随意选择一个岔路口(边)，当发现走不通的时候，回退到上过岔路口，重新选择一条路继续走，知道找到最终的出口。

借助递归，实现深度优先搜索并不困难。

**时间复杂度**：O(E)，E为边的个数。每条边最多被访问两次。

**空间复杂度**：O(V)，V为顶点个数。visited、prev数组大小与顶点个数成正比。























## 题目汇总

### 算法相关

#### 贪心算法

找零钱问题

+ [860. 柠檬水找零](https://leetcode-cn.com/problems/lemonade-change)

区域选择问题

+ [435. 无重叠区间](https://leetcode-cn.com/problems/non-overlapping-intervals)
+ [452. 用最少数量的箭引爆气球](https://leetcode-cn.com/problems/minimum-number-of-arrows-to-burst-balloons)
+ [56. 合并区间](https://leetcode-cn.com/problems/merge-intervals)

跳跃问题

+ [55. 跳跃游戏](https://leetcode-cn.com/problems/jump-game)
+ [45. 跳跃游戏 II](https://leetcode-cn.com/problems/jump-game-ii)

#### 分治算法

+ [剑指 Offer 51. 数组中的逆序对](https://leetcode-cn.com/problems/shu-zu-zhong-de-ni-xu-dui-lcof/)

#### 回溯算法

排列问题

+ [17. 电话号码的字母组合](https://leetcode-cn.com/problems/letter-combinations-of-a-phone-number)
+ [93. 复原 IP 地址](https://leetcode-cn.com/problems/restore-ip-addresses)
+ [131. 分割回文串](https://leetcode-cn.com/problems/palindrome-partitioning)
+ [46. 全排列](https://leetcode-cn.com/problems/permutations)
+ [47. 全排列 II](https://leetcode-cn.com/problems/permutations-ii)

组合问题

+ [77. 组合](https://leetcode-cn.com/problems/combinations)
+ [39. 组合总和](https://leetcode-cn.com/problems/combination-sum)
+ [40. 组合总和 II](https://leetcode-cn.com/problems/combination-sum-ii)
+ [216. 组合总和 III](https://leetcode-cn.com/problems/combination-sum-iii)
+ [78. 子集](https://leetcode-cn.com/problems/subsets)
+ [90. 子集 II](https://leetcode-cn.com/problems/subsets-ii)
+ [401. 二进制手表](https://leetcode-cn.com/problems/binary-watch)

二维问题

+ [79. 单词搜索](https://leetcode-cn.com/problems/word-search)
+ [200. 岛屿数量](https://leetcode-cn.com/problems/number-of-islands)
+ [130. 被围绕的区域](https://leetcode-cn.com/problems/surrounded-regions)
+ [417. 太平洋大西洋水流](https://leetcode-cn.com/problems/pacific-atlantic-water-flow)
+ [51. N 皇后](https://leetcode-cn.com/problems/n-queens)
+ [52. N皇后 II](https://leetcode-cn.com/problems/n-queens-ii)
+ [37. 解数独](https://leetcode-cn.com/problems/sudoku-solver)

### 数据结构相关

#### 链表

单链表反转

链表中环的检测

两个有序的链表合并

删除链表倒数第 n 个结点

求链表的中间结点

#### 栈

leetcode20,155,232,844,224,682,496.  

表达式求值、括号匹配

#### 堆

+ [295. 数据流的中位数](https://leetcode-cn.com/problems/find-median-from-data-stream)





